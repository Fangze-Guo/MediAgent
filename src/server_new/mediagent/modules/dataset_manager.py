# mediagent/modules/dataset_manager.py
from __future__ import annotations
"""
dataset_manager.py â€”â€” DEMO ç‰ˆï¼ˆå¼‚æ­¥ï¼‰
- è¡¨ï¼šdataset_catalogï¼ˆ9ä¸ªå­—æ®µï¼‰
- æ¥å£ï¼š
    async overview() -> {"ok": bool, "text": str}
    async focus(dataset_name, user_need_text) -> {"ok": bool, "text": str}

ç‰¹æ€§ï¼š
- DBï¼šä¼˜å…ˆ aiosqliteï¼Œç¼ºå¤±åˆ™ç”¨ asyncio.to_thread + sqlite3
- LLMï¼šä¼˜å…ˆ AsyncOpenAIï¼Œç¼ºå¤±/æœªé…åˆ™é™çº§æœ¬åœ°æ‘˜è¦
- æ–‡ä»¶è¯»å–ï¼šæ”¾åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼Œé¿å…é˜»å¡
"""

import csv
import io
import json
import os
import sqlite3
import asyncio
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from mediagent.paths import in_data

# ===================== éœ€é…ç½®è·¯å¾„ =====================
DATASET_FILES_ROOT: Path = in_data("files","public","dataset_info")
DATASET_DB_PATH: Path = in_data("db","app.sqlite3")

# ===================== LLMï¼ˆOpenAI å…¼å®¹ç½‘å…³ï¼‰ =====================
LLM_MODEL: Optional[str] = "deepseek-chat"
LLM_BASE_URL: Optional[str] = "https://api.deepseek.com/v1"
LLM_API_KEY: Optional[str] = "sk-d0e27c4c590a454e8284309067c03f04"
LLM_TIMEOUT: float = 60.0

# ===================== è¯»å–ä¸å¤§å°æ§åˆ¶ =====================
MAX_TEXT_BYTES_FOR_LLM: int = 2 * 1024 * 1024  # ä¿ç•™å¸¸é‡
CSV_ENCODING_CANDIDATES: Tuple[str, ...] = ("utf-8-sig", "utf-8", "gbk", "latin-1")
SAMPLE_HEAD_ROWS: int = 200
LOCAL_SUMMARY_ROWS: int = 50

# ğŸ”¹ Map-Reduce è§¦å‘ä¸åˆ†å—ç­–ç•¥ï¼ˆä¸æ”¹æç¤ºè¯ï¼Œä»…å†…éƒ¨æ§åˆ¶ï¼‰
SINGLEPASS_MAX_BYTES: int = 512 * 1024      # è¶…è¿‡åˆ™èµ°åˆ†å—
CHUNK_TARGET_BYTES: int = 256 * 1024        # æ¯å—ç›®æ ‡å¤§å°ï¼ˆæŒ‰è¡Œå¯¹é½ï¼‰
CHUNK_HARD_MAX_BYTES: int = 320 * 1024      # å•å—ç¡¬ä¸Šé™ï¼ˆè¡Œå¤ªé•¿æ—¶å…œåº•ï¼‰

# ===================== è¡¨ç»“æ„ï¼ˆè¯´æ˜ï¼‰ =====================
"""
CREATE TABLE IF NOT EXISTS dataset_catalog (
  dataset_name         TEXT PRIMARY KEY,
  case_count           INTEGER,
  clinical_data_desc   TEXT,
  text_data_desc       TEXT,
  imaging_data_desc    TEXT,
  pathology_data_desc  TEXT,
  genomics_data_desc   TEXT,
  annotation_desc      TEXT,
  notes                TEXT
);
"""

# ===================== æ•°æ®ç±» =====================

@dataclass
class CatalogRow:
    dataset_name: str
    case_count: Optional[int]
    clinical_data_desc: Optional[str]
    text_data_desc: Optional[str]
    imaging_data_desc: Optional[str]
    pathology_data_desc: Optional[str]
    genomics_data_desc: Optional[str]
    annotation_desc: Optional[str]
    notes: Optional[str]

# ===================== å¯¹å¤–æ¥å£ï¼ˆå¼‚æ­¥ï¼‰ =====================

async def overview(
    *,
    db_path: Optional[Path] = None,
    limit: Optional[int] = None,
) -> Dict[str, Any]:
    try:
        rows = await _fetch_catalog_rows_async(db_path or DATASET_DB_PATH, limit=limit)
        if not rows:
            return {"ok": True, "text": "å½“å‰æ•°æ®åº“ä¸­æ²¡æœ‰å¯ç”¨çš„æ•°æ®é›†æ¡ç›®ã€‚"}

        parts: List[str] = [f"å…±æ”¶å½•æ•°æ®é›† {len(rows)} ä¸ªã€‚"]
        for i, r in enumerate(rows, 1):
            cc = f"{r.case_count}ä¾‹" if isinstance(r.case_count, int) else "ä¾‹æ•°æœªçŸ¥"
            im = _short(r.imaging_data_desc)
            tx = _short(r.text_data_desc)
            pa = _short(r.pathology_data_desc)
            ge = _short(r.genomics_data_desc)
            an = _short(r.annotation_desc)
            parts.append(
                f"{i}. {r.dataset_name}ï¼š{cc}ï¼›å½±åƒ[{im}]ï¼›æ–‡æœ¬[{tx}]ï¼›ç—…ç†[{pa}]ï¼›åŸºå› [{ge}]ï¼›æ ‡æ³¨[{an}]ã€‚"
            )
        return {"ok": True, "text": "\n".join(parts)}
    except Exception as e:
        return {"ok": False, "text": f"è¯»å– catalog å¤±è´¥ï¼š{e!r}"}


async def focus(
    dataset_name: str,
    user_need_text: str,
    *,
    db_path: Optional[Path] = None,  # é¢„ç•™
) -> Dict[str, Any]:
    """
    å¼‚æ­¥å®šç‚¹æŸ¥è¯¢ï¼š
    - å®šä½å¹¶è¯»å– <dataset_name>.csv/.xlsxï¼ˆä¼˜å…ˆ CSVï¼‰
    - å°†â€œå®Œæ•´è¡¨æ ¼å†…å®¹â€+ æç¤º + ç”¨æˆ·éœ€æ±‚äº¤ç»™ LLM
    - å½“å…¨æ–‡è¿‡å¤§æ—¶ï¼Œè‡ªåŠ¨ Map-Reduceï¼ˆä¸æ”¹æç¤ºè¯ï¼‰ï¼Œç¡®ä¿è¦†ç›–å…¨éƒ¨å†…å®¹
    - å¤±è´¥/æœªé…åˆ™æœ¬åœ°å¯å‘å¼æ‘˜è¦
    """
    try:
        # 1) å®šä½æ–‡ä»¶
        path, fmt = await asyncio.to_thread(_locate_file, dataset_name)
        if path is None:
            return {"ok": False, "text": f"æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼š{dataset_name}.csv/.xlsx"}

        # 2) è¯»å–å®Œæ•´æ–‡æœ¬
        if fmt == "csv":
            full_text, _trunc, info = await asyncio.to_thread(_read_csv_full_text, path)
        else:
            full_text, _trunc, info = await asyncio.to_thread(_read_xlsx_full_tsv, path)

        # 3) ç»„ç»‡ä½ çš„â€œå›ºå®šæç¤ºè¯â€ï¼ˆä¿æŒåŸæ ·ï¼‰
        system_prompt = (
            "ä½ æ˜¯åŒ»ç–—æ•°æ®åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç»™å®šçš„è¡¨æ ¼å†…å®¹ä¸ç”¨æˆ·éœ€æ±‚ï¼Œè¾“å‡ºä¸€æ®µç®€çŸ­æ€»ç»“ï¼ˆ100~200å­—ï¼‰ï¼Œè¡¨æ ¼æ˜¯å¯¹åº”åŒåæ•°æ®é›†çš„ä¿¡æ¯è¡¨ï¼Œç”¨æˆ·æƒ³äº†è§£çš„æ˜¯æ•°æ®é›†çš„æƒ…å†µï¼Œä½ å¯ä»¥ä»è¡¨æ ¼å‡ºå‘è¿›è¡Œåˆç†æ¨æµ‹ã€‚"
            "è¦æ±‚ï¼š1) ä¸è¦é€è¡Œç½—åˆ—åŸå§‹æ•°æ®ï¼›2) æç‚¼åˆ—åã€å…³é”®ä¿¡æ¯ä¸å¯ç”¨æ€§ï¼›3) è‹¥ä¿¡æ¯ä¸è¶³ï¼Œè¯´æ˜ç¼ºå£ï¼›"
            "4) ä¸­æ–‡è¾“å‡ºï¼›5) ä¸åŒ…å«è¡¨æ ¼æºç ã€‚"
        )

        # 4) åˆ¤æ–­æ˜¯å¦éœ€è¦ Map-Reduce
        total_bytes = len(full_text.encode("utf-8"))
        if await _llm_available_async():
            if total_bytes <= SINGLEPASS_MAX_BYTES:
                # å•è½®ç›´å‡ºï¼ˆå®Œæ•´æ–‡æœ¬ï¼‰
                user_prompt = _build_user_prompt_for_llm(
                    dataset_name=dataset_name,
                    user_need_text=user_need_text,
                    file_text=full_text,
                    truncated=False,
                    file_info={**info, "pipeline": "single-pass", "bytes": total_bytes},
                )
                out = await _call_llm_async(system_prompt, user_prompt)
                if out:
                    return {"ok": True, "text": out.strip()}
            else:
                # ğŸ”¹ Map-Reduce è·¯å¾„ï¼ˆä¸æ”¹æç¤ºè¯ï¼‰
                merged = await _map_reduce_summarize(
                    system_prompt=system_prompt,
                    dataset_name=dataset_name,
                    user_need_text=user_need_text,
                    full_text=full_text,
                    base_file_info=info,
                )
                if merged:
                    return {"ok": True, "text": merged.strip()}

        # 5) LLM ä¸å¯ç”¨æˆ–å¤±è´¥ï¼šæœ¬åœ°æ‘˜è¦
        local_text = await asyncio.to_thread(
            _local_summarize, dataset_name, user_need_text, full_text, info, max_rows=LOCAL_SUMMARY_ROWS
        )
        return {"ok": True, "text": local_text}
    except Exception as e:
        return {"ok": False, "text": f"å¤„ç†å¤±è´¥ï¼š{e!r}"}

# ===================== DB è¯»å–ï¼ˆä¼˜å…ˆ aiosqliteï¼‰ =====================

async def _fetch_catalog_rows_async(db_path: Path, limit: Optional[int]) -> List[CatalogRow]:
    db_path = Path(db_path).expanduser().resolve()
    if not db_path.exists():
        raise FileNotFoundError(f"catalog æ•°æ®åº“ä¸å­˜åœ¨ï¼š{db_path}")

    try:
        import aiosqlite  # type: ignore
    except Exception:
        return await asyncio.to_thread(_fetch_catalog_rows_sync, db_path, limit)

    sql = (
        "SELECT dataset_name, case_count, clinical_data_desc, text_data_desc, "
        "imaging_data_desc, pathology_data_desc, genomics_data_desc, annotation_desc, notes "
        "FROM dataset_catalog "
        "ORDER BY dataset_name ASC"
    )
    rows: List[CatalogRow] = []
    async with aiosqlite.connect(db_path.as_posix()) as conn:
        async with conn.execute(sql) as cur:
            i = 0
            async for dn, cc, cl, tx, im, pa, ge, an, no in cur:
                rows.append(
                    CatalogRow(
                        dataset_name=str(dn),
                        case_count=int(cc) if cc is not None else None,
                        clinical_data_desc=_nz(cl),
                        text_data_desc=_nz(tx),
                        imaging_data_desc=_nz(im),
                        pathology_data_desc=_nz(pa),
                        genomics_data_desc=_nz(ge),
                        annotation_desc=_nz(an),
                        notes=_nz(no),
                    )
                )
                i += 1
                if limit is not None and i >= limit:
                    break
    return rows


def _fetch_catalog_rows_sync(db_path: Path, limit: Optional[int]) -> List[CatalogRow]:
    sql = (
        "SELECT dataset_name, case_count, clinical_data_desc, text_data_desc, "
        "imaging_data_desc, pathology_data_desc, genomics_data_desc, annotation_desc, notes "
        "FROM dataset_catalog "
        "ORDER BY dataset_name ASC"
    )
    rows: List[CatalogRow] = []
    with sqlite3.connect(db_path.as_posix(), check_same_thread=False) as conn:
        cur = conn.execute(sql)
        for i, (dn, cc, cl, tx, im, pa, ge, an, no) in enumerate(cur):
            rows.append(
                CatalogRow(
                    dataset_name=str(dn),
                    case_count=int(cc) if cc is not None else None,
                    clinical_data_desc=_nz(cl),
                    text_data_desc=_nz(tx),
                    imaging_data_desc=_nz(im),
                    pathology_data_desc=_nz(pa),
                    genomics_data_desc=_nz(ge),
                    annotation_desc=_nz(an),
                    notes=_nz(no),
                )
            )
            if limit is not None and i + 1 >= limit:
                break
    return rows

# ===================== æ–‡ä»¶å®šä½ä¸è¯»å– =====================

def _locate_file(dataset_name: str) -> Tuple[Optional[Path], Optional[str]]:
    root = DATASET_FILES_ROOT.expanduser().resolve()
    csv_path = root / f"{dataset_name}.csv"
    xlsx_path = root / f"{dataset_name}.xlsx"
    if csv_path.exists():
        return csv_path, "csv"
    if xlsx_path.exists():
        return xlsx_path, "xlsx"
    return None, None


def _detect_csv_encoding(p: Path) -> str:
    for enc in CSV_ENCODING_CANDIDATES:
        try:
            with p.open("r", encoding=enc) as f:
                f.read(1024)
            return enc
        except Exception:
            continue
    return "utf-8"

def _read_csv_full_text(p: Path) -> Tuple[str, bool, Dict[str, Any]]:
    enc = _detect_csv_encoding(p)
    txt = p.read_text(encoding=enc)
    return txt, False, {"format": "csv", "encoding": enc, "size_bytes": p.stat().st_size, "note": "full_text"}

def _read_xlsx_full_tsv(p: Path) -> Tuple[str, bool, Dict[str, Any]]:
    try:
        import openpyxl  # type: ignore
    except Exception:
        msg = f"(æç¤º) æœªå®‰è£… openpyxlï¼Œæ— æ³•è¯»å– {p.name} çš„å†…å®¹ã€‚è¯·å®‰è£… openpyxlã€‚"
        return msg, False, {"format": "xlsx", "error": "openpyxl_missing", "size_bytes": p.stat().st_size}

    from openpyxl import load_workbook  # type: ignore
    wb = load_workbook(p.as_posix(), read_only=True, data_only=True)
    sheetnames = wb.sheetnames
    if not sheetnames:
        try:
            wb.close()
        except Exception:
            pass
        return "(ç©ºè¡¨)", False, {"format": "xlsx", "sheets": 0}
    ws = wb[sheetnames[0]]

    out = io.StringIO()
    for row in ws.iter_rows(values_only=True):
        cells = [("" if v is None else str(v)).replace("\r", " ").replace("\n", " ") for v in row]
        out.write("\t".join(cells) + "\n")
    tsv = out.getvalue()
    try:
        wb.close()
    except Exception:
        pass
    return tsv, False, {"format": "xlsx", "sheet": sheetnames[0], "size_bytes": p.stat().st_size, "note": "full_text"}

# ===================== LLMï¼ˆå¼‚æ­¥ï¼‰ä¸é™çº§æ‘˜è¦ =====================

async def _llm_available_async() -> bool:
    if not LLM_MODEL:
        return False
    try:
        from openai import AsyncOpenAI  # type: ignore
    except Exception:
        return False
    return True

async def _call_llm_async(system_prompt: str, user_prompt: str) -> Optional[str]:
    try:
        from openai import AsyncOpenAI  # type: ignore
        client = AsyncOpenAI(
            api_key=(LLM_API_KEY or "lm-studio"),
            base_url=(LLM_BASE_URL or None),
            timeout=LLM_TIMEOUT,
        )
        resp = await client.chat.completions.create(
            model=LLM_MODEL,
            temperature=0.2,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
        )
        txt = (resp.choices[0].message.content or "").strip()
        return txt or None
    except Exception:
        return None

def _build_user_prompt_for_llm(
    *,
    dataset_name: str,
    user_need_text: str,
    file_text: str,
    truncated: bool,
    file_info: Dict[str, Any],
) -> str:
    info = json.dumps(file_info, ensure_ascii=False)
    tip = "ï¼ˆä»¥ä¸‹å†…å®¹å¯èƒ½ä¸ºé‡‡æ ·ç‰‡æ®µï¼‰" if truncated else "ï¼ˆä»¥ä¸‹ä¸ºå®Œæ•´å†…å®¹æˆ–è¾ƒå®Œæ•´ç‰‡æ®µï¼‰"
    return (
        f"ã€ç”¨æˆ·éœ€æ±‚ã€‘\n{user_need_text}\n\n"
        f"ã€æ•°æ®é›†åã€‘\n{dataset_name}\n\n"
        f"ã€æ–‡ä»¶ä¿¡æ¯ã€‘\n{info}\n\n"
        f"ã€è¡¨æ ¼æ–‡æœ¬ã€‘{tip}\n{file_text}"
    )

def _local_summarize(
    dataset_name: str,
    user_need_text: str,
    file_text: str,
    file_info: Dict[str, Any],
    *,
    max_rows: int = 50,
) -> str:
    lines = [ln for ln in (file_text or "").splitlines() if ln.strip()]
    head = lines[:max_rows]
    delimiter = "\t" if ("\t" in head[0] if head else False) else ","
    cols = head[0].split(delimiter) if head else []
    num_cols = len(cols)
    sample_hint = f"å·²é‡‡æ ·å‰{len(head)}è¡Œ" if head else "æœªèƒ½é‡‡æ ·"
    info_hint = f"æ ¼å¼ {file_info.get('format')}, ä¼°è®¡å¤§å° {file_info.get('size_bytes','?')} å­—èŠ‚"
    cols_short = ", ".join(cols[:10]) + ("..." if len(cols) > 10 else "")
    return (
        f"æ•°æ®é›† {dataset_name} çš„ç®€è¦æ‘˜è¦ï¼ˆæœ¬åœ°ä¼°è®¡ï¼‰:\n"
        f"- {info_hint}ï¼›{sample_hint}\n"
        f"- ä¼°è®¡åˆ—æ•°ï¼š{num_cols}ï¼›éƒ¨åˆ†åˆ—åï¼š{cols_short if cols else 'æœªçŸ¥'}\n"
        f"- ä½ çš„éœ€æ±‚ï¼š{user_need_text}\n"
        f"è‹¥éœ€æ›´è¯¦ç»†æ‘˜è¦ï¼Œè¯·é…ç½® DATASET_LLM_* å¹¶/æˆ–ç¼©å°æ•°æ®èŒƒå›´ã€‚"
    )

# ğŸ”¹ Map-Reduce å®ç°ï¼ˆä¸æ”¹ä½ çš„æç¤ºè¯ï¼‰ =====================

def _split_text_by_bytes_linesafe(text: str, target_bytes: int, hard_max: int) -> List[str]:
    """æŒ‰å­—èŠ‚å¤§å°åˆ‡åˆ†ï¼Œå°½é‡åœ¨è¡Œè¾¹ç•Œå¤„æ–­å¼€ã€‚"""
    lines = text.splitlines(keepends=True)
    chunks: List[str] = []
    buf: List[str] = []
    buf_bytes = 0
    for ln in lines:
        ln_b = len(ln.encode("utf-8"))
        if buf and (buf_bytes + ln_b > target_bytes):
            # å¦‚æœå•è¡Œè¶…é•¿ä¸”ç¼“å†²ä¸ºç©ºï¼Œç¡¬åˆ‡
            if buf_bytes == 0 and ln_b > hard_max:
                chunks.append(ln[:hard_max].encode("utf-8", "ignore").decode("utf-8", "ignore"))
                rest = ln[hard_max:]
                if rest:
                    lines.insert(0, rest)  # ç»§ç»­å¤„ç†å‰©ä½™éƒ¨åˆ†
                continue
            chunks.append("".join(buf))
            buf, buf_bytes = [ln], ln_b
        else:
            buf.append(ln)
            buf_bytes += ln_b
        if buf_bytes >= hard_max:
            chunks.append("".join(buf))
            buf, buf_bytes = [], 0
    if buf:
        chunks.append("".join(buf))
    return chunks

async def _map_reduce_summarize(
    *,
    system_prompt: str,
    dataset_name: str,
    user_need_text: str,
    full_text: str,
    base_file_info: Dict[str, Any],
) -> Optional[str]:
    # åˆ‡å—
    chunks = _split_text_by_bytes_linesafe(full_text, CHUNK_TARGET_BYTES, CHUNK_HARD_MAX_BYTES)
    if not chunks:
        return None

    # Map é˜¶æ®µï¼šæ¯å—å„è‡ªè°ƒç”¨ä¸€æ¬¡åŒæ ·çš„æç¤ºè¯
    part_summaries: List[str] = []
    for idx, chunk in enumerate(chunks, start=1):
        file_info = {
            **base_file_info,
            "pipeline": "map-reduce",
            "stage": "map",
            "chunk_index": idx,
            "chunks_total": len(chunks),
            "bytes": len(chunk.encode("utf-8")),
        }
        user_prompt = _build_user_prompt_for_llm(
            dataset_name=dataset_name,
            user_need_text=user_need_text,
            file_text=chunk,
            truncated=False,
            file_info=file_info,
        )
        out = await _call_llm_async(system_prompt, user_prompt)
        if not out:
            continue
        part_summaries.append(f"[ç¬¬{idx}/{len(chunks)}å—æ‘˜è¦]\n{out.strip()}")

    if not part_summaries:
        return None

    # Reduce é˜¶æ®µï¼šæŠŠæ‰€æœ‰åˆ†å—æ‘˜è¦åˆå¹¶ä¸ºâ€œæ–‡ä»¶æ–‡æœ¬â€ï¼Œå†æ¬¡è°ƒç”¨åŒæ ·çš„æç¤ºè¯ç”Ÿæˆæœ€ç»ˆæ€»ç»“
    merged_text = "\n\n".join(part_summaries)
    reduce_info = {
        **base_file_info,
        "pipeline": "map-reduce",
        "stage": "reduce",
        "chunks_total": len(chunks),
        "bytes": len(merged_text.encode("utf-8")),
    }
    final_user_prompt = _build_user_prompt_for_llm(
        dataset_name=dataset_name,
        user_need_text=user_need_text,
        file_text=merged_text,
        truncated=False,
        file_info=reduce_info,
    )
    final = await _call_llm_async(system_prompt, final_user_prompt)
    return final

# ===================== å·¥å…·å‡½æ•° =====================

def _nz(x: Optional[str]) -> Optional[str]:
    if x is None:
        return None
    s = str(x).strip()
    return s if s else None

def _short(x: Optional[str], n: int = 120) -> str:
    if not x:
        return "æ— "
    s = str(x).strip().replace("\n", " ")
    return (s[:n] + "â€¦") if len(s) > n else s

def _safe_cell(v: Any) -> str:
    s = "" if v is None else str(v)
    return s.replace("\r", " ").replace("\n", " ")

# ===================== å‘½ä»¤è¡Œï¼ˆå¼‚æ­¥ï¼‰ =====================

async def _amain(argv: Optional[List[str]] = None) -> int:
    import argparse
    p = argparse.ArgumentParser(description="dataset_manager DEMOï¼ˆoverview / focus, asyncï¼‰")
    sub = p.add_subparsers(dest="cmd", required=True)

    p1 = sub.add_parser("overview", help="è¯»å– catalog å¹¶ç”Ÿæˆæ€»è§ˆæ–‡æœ¬")
    p1.add_argument("--db", type=str, default=str(DATASET_DB_PATH), help="SQLite DB è·¯å¾„")
    p1.add_argument("--limit", type=int, default=None, help="æœ€å¤šè¯»å–çš„æ¡æ•°")

    p2 = sub.add_parser("focus", help="é’ˆå¯¹æ•°æ®é›†åšå®šç‚¹æŸ¥è¯¢å¹¶ç”ŸæˆçŸ­æ‘˜è¦")
    p2.add_argument("--name", required=True, help="æ•°æ®é›†åï¼ˆåŒå csv/xlsxï¼‰")
    p2.add_argument("--need", required=True, help="ç”¨æˆ·æœ¬æ¬¡éœ€æ±‚ï¼ˆè‡ªç„¶è¯­è¨€ï¼‰")

    args = p.parse_args(argv)
    if args.cmd == "overview":
        res = await overview(db_path=Path(args.db), limit=args.limit)
    else:
        res = await focus(dataset_name=args.name, user_need_text=args.need)

    print(json.dumps(res, ensure_ascii=False, indent=2))
    return 0


if __name__ == "__main__":
    asyncio.run(_amain())
