from radiomics.featureextractor import RadiomicsFeatureExtractor
import SimpleITK as sitk
import numpy as np
import pandas as pd
from typing import Dict, Any
from loguru import logger
from pathlib import Path
import pickle
from copy import deepcopy

from hiomics.utils import parallel, generate_spatial_bounding_box, load_params
import hiomics.preprocessing as preprocessing
import hiomics.decomposition as decomposition
from hiomics.task.step import AbcStep
from hiomics.task.data import PathData, FeatureData


def transform_to_wide_format(df, id_column, sr_id_column, feature_columns):
    feat_df = df[[id_column, sr_id_column] + feature_columns]
    melted = feat_df.melt(id_vars=[id_column, sr_id_column], 
                         var_name='feature_name', 
                         value_name='feature_value')
    melted.sort_values(by=[id_column, sr_id_column], inplace=True)
    melted['new_col'] = melted[sr_id_column].astype(str) + '@' + melted['feature_name']

    result = melted.pivot_table(
        index=id_column, 
        columns='new_col', 
        values='feature_value',
        aggfunc='first'
    ).reset_index()
    # result.fillna(0, inplace=True)
    
    new_feature_columns = result.columns.tolist()[1:]
    return result, new_feature_columns

def radiomics_worker(extractor: RadiomicsFeatureExtractor, 
                     image_path: str, 
                     mask_path: str,
                     ) -> Dict[str, Any]:
    image_sitk = sitk.ReadImage(image_path)
    mask_sitk = sitk.ReadImage(mask_path)
    image_np = sitk.GetArrayFromImage(image_sitk)
    mask_np = sitk.GetArrayFromImage(mask_sitk)
    if mask_np.sum() == 0:
        logger.warning(f"{mask_path} has no foreground pixels")
        return None, None
    mask_bin_np = np.where(mask_np > 0, 1, 0)

    roi_start, roi_end = generate_spatial_bounding_box(mask_bin_np)
    """ ä¸èƒ½åˆ‡roi, è¿™ä¼šå¯¼è‡´normalizationæœ‰é—®é¢˜ """
    # image_roi_np = image_np[roi_start[0]:roi_end[0], roi_start[1]:roi_end[1], roi_start[2]:roi_end[2]]
    # mask_roi_np = mask_np[roi_start[0]:roi_end[0], roi_start[1]:roi_end[1], roi_start[2]:roi_end[2]]
    image_roi_np = image_np
    mask_roi_np = mask_np

    labels = [int(_) for _ in np.unique(mask_roi_np) if _ > 0] # only keep non-background labels
    features = []
    errors = []
    for i in labels:
        mask_roi_i_np = np.where(mask_roi_np == i, 1, 0)
        if mask_roi_i_np.sum() < 5:
            logger.warning(f"{mask_path} has less than 5 pixels for label {i}, filling with zeros")
            errors.append(f"Label {i} has less than 5 pixels")
            features.append({})
            continue
        features_i_raw = extractor.execute(sitk.GetImageFromArray(image_roi_np), sitk.GetImageFromArray(mask_roi_i_np))
        features_i = {key: features_i_raw[key] for key in features_i_raw.keys() if not key.startswith("diagnostics_")}
        features.append(features_i)
        errors.append(None)

    features_df = pd.DataFrame(features)
    feature_columns = features_df.columns.tolist()
    label_df = pd.DataFrame(labels, columns=["sr_id"])
    label_df["sr_id"] = label_df["sr_id"].apply(lambda x: f"SR_{x}")
    error_df = pd.DataFrame(errors, columns=["Error"])
    ret_df = pd.concat([label_df, error_df, features_df], axis=1)
    return ret_df, feature_columns

class HiomicsRadiomicsLHCP:
    def __init__(self, input_image_paths, input_mask_paths, **kwargs):
        self.input_image_paths = input_image_paths
        self.input_mask_paths = input_mask_paths
        self.params = kwargs

        self.sr_features_df = None
        self.sr_feature_columns = None
        self.case_features_df = None
        self.case_feature_columns = None
        self.sr_features_df_before_fillna = None
        self.sr_features_df_after_fillna = None

    def debug_pkl(self):
        data = {
            "input_image_paths": self.input_image_paths,
            "input_mask_paths": self.input_mask_paths,
            "params": self.params,
            "sr_features_df": self.sr_features_df,
            "sr_feature_columns": self.sr_feature_columns,
            "case_features_df": self.case_features_df,
            "case_feature_columns": self.case_feature_columns,
            "sr_features_df_before_fillna": self.sr_features_df_before_fillna,
            "sr_features_df_after_fillna": self.sr_features_df_after_fillna,
        }
        with open("radiomics_lhcp.pkl", "wb") as f:
            pickle.dump(data, f)

    def load_pkl(self):
        with open("radiomics_lhcp.pkl", "rb") as f:
            data = pickle.load(f)
        for k, v in data.items():
            setattr(self, k, v)


    def get_radiomics(self):
        assert self.params.get("radiomics_params", None) is not None, "radiomics_params is not set"
        n_jobs = self.params.get("n_jobs", 1)

        extractor = RadiomicsFeatureExtractor(self.params.get("radiomics_params"))
        kwargs_list = []
        for idx, (image_path, mask_path) in enumerate(zip(self.input_image_paths, self.input_mask_paths)):
            kwargs_list.append(dict(
                extractor=extractor,
                image_path=image_path,
                mask_path=mask_path,
            ))
        ret = parallel.worker(radiomics_worker, kwargs_list, use_multiprocessing=n_jobs > 1, desc="Hiomics Radiomics", max_workers=n_jobs, parallel_type="process")

        feats = []
        for i, (df, feature_columns) in enumerate(ret):
            if df is None:
                logger.warning(f"No features extracted for {self.input_mask_paths[i]}")
                continue
            self.sr_feature_columns = feature_columns
            df.insert(0, "unique_id", f"{i}")
            feats.append(df)
        self.sr_features_df = pd.concat(feats, axis=0, ignore_index=True)
        # self.sr_features_df.fillna(self.sr_features_df.mean(numeric_only=True), inplace=True)
        # self.sr_features_df.fillna(0, inplace=True)

        """
        @WZT: case_features_df ä¸æ˜¯æ¯ä¸ªcaseéƒ½æœ‰ï¼Œå› ä¸ºæœ‰çš„caseæ˜¯æ²¡æœ‰tumorçš„ï¼Œæ‰€ä»¥ len(case_features_df) < len(self.input_image_paths)
        """
        self.case_features_df, self.case_feature_columns = transform_to_wide_format(self.sr_features_df, "unique_id", "sr_id", self.sr_feature_columns)
        self.case_features_df["unique_id"] = self.case_features_df["unique_id"].astype(int)
        self.case_features_df.sort_values(by="unique_id", inplace=True)
        # self.case_features_df.fillna(self.case_features_df.mean(numeric_only=True), inplace=True)
        # self.case_features_df.fillna(0, inplace=True)

        case_info_df = pd.DataFrame(range(len(self.input_image_paths)), columns=["unique_id"])
        logger.debug(f"case_info_df: {len(case_info_df)}, case_features_df: {len(self.case_features_df)}")
        self.case_features_df = pd.merge(case_info_df, self.case_features_df, on="unique_id", how="left")

        # from IPython import embed; embed()

        return self.case_features_df[self.case_feature_columns].copy()

    def get_base(self):
        """è·å–åŸºç¡€radiomicsç‰¹å¾ï¼ˆbase featuresï¼‰"""
        return self.get_radiomics()
    
    def get_primary(self):

        if self.sr_feature_columns is None:
            self.get_radiomics()

        # Get primary params first
        primary_params = self.params.get("primary_params", {})
        
        # Get preprocessing and decomposition parameters
        prep_args = primary_params.get("preprocessing", {"method": "StandardScaler", "params": None})
        decomp_args = primary_params.get("decomposition", {"method": "PCA", "params": {"random_state": 42}})
        
        # Work with SR level features
        sr_features_filled = self.sr_features_df.copy()
        
        # Store data before fillna
        self.sr_features_df_before_fillna = sr_features_filled.copy()
        
        # sr_features_filled[self.sr_feature_columns] = sr_features_filled[self.sr_feature_columns].fillna(
        #     sr_features_filled[self.sr_feature_columns].mean(numeric_only=True)
        # )
        sr_features_filled[self.sr_feature_columns] = sr_features_filled[self.sr_feature_columns].fillna(0)
        
        # Store data after fillna
        self.sr_features_df_after_fillna = sr_features_filled.copy()
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥ä½¿ç”¨å·²æœ‰æ¨¡å‹(åŸºäºforceå‚æ•°ä¼ é€’çš„é€»è¾‘)
        should_train_new = self.params.get("should_train_new", True)
        
        if should_train_new:
            # è®­ç»ƒæ–°æ¨¡å‹
            
            # Preprocessing model - fit on all SR features
            prep_model = preprocessing.get_method(prep_args["method"])(params=prep_args["params"])
            prep_model.fit(sr_features_filled[self.sr_feature_columns])
            
            # Save preprocessing model if path is provided
            if "preprocessing_pkl_path" in primary_params:
                prep_model.save(primary_params["preprocessing_pkl_path"])
                logger.info(f"Saved preprocessing model to {primary_params['preprocessing_pkl_path']}")
            
            # Apply preprocessing
            norm_feat_np = prep_model.predict(sr_features_filled[self.sr_feature_columns])
            norm_sr_feat_df = pd.DataFrame(norm_feat_np, columns=self.sr_feature_columns)
            
            # Get unique SR IDs
            unique_sr_ids = sr_features_filled["sr_id"].unique()
            
            # Create decomposition models for each SR
            decomp_models = {}
            decomp_features_list = []
            
            for sr_id in unique_sr_ids:
                # Get features for this SR
                sr_mask = sr_features_filled["sr_id"] == sr_id
                sr_specific_features = norm_sr_feat_df[sr_mask]
                
                # Create and fit decomposition model for this SR
                decomp_model = decomposition.get_method(decomp_args["method"])(params=decomp_args["params"])
                decomp_model.fit(sr_specific_features)
                decomp_models[sr_id] = decomp_model
                
                # Apply decomposition
                decomp_feat = decomp_model.predict(sr_specific_features)
                
                # Add SR ID and unique ID back
                decomp_feat_with_ids = pd.concat([
                    sr_features_filled[sr_mask][["unique_id", "sr_id"]].reset_index(drop=True),
                    decomp_feat.reset_index(drop=True)
                ], axis=1) # éƒ½æ˜¯åŸºäº sr_mask çš„ï¼Œæ‰€ä»¥å¯ä»¥concat
                decomp_features_list.append(decomp_feat_with_ids)
            
            # Combine all SR decomposed features
            all_decomp_features = pd.concat(decomp_features_list, axis=0, ignore_index=True)
            
            # Save decomposition models if path is provided
            if "decomposition_pkl_path" in primary_params:
                with open(primary_params["decomposition_pkl_path"], "wb") as f:
                    pickle.dump(decomp_models, f)
                logger.info(f"Saved decomposition models to {primary_params['decomposition_pkl_path']}")
            
            # Store models for potential later use
            self.prep_model = prep_model
            self.decomp_models = decomp_models
            
            # Transform to wide format
            decomp_columns = [col for col in all_decomp_features.columns if col not in ["unique_id", "sr_id"]]
            result_df, _ = transform_to_wide_format(all_decomp_features, "unique_id", "sr_id", decomp_columns)
            
            # return result_df
            
        else:
            # åŠ è½½å¹¶ä½¿ç”¨å·²æœ‰æ¨¡å‹
            
            # Load preprocessing model
            assert "preprocessing_pkl_path" in primary_params, "preprocessing_pkl_path is required when using existing models"
            try:
                prep_model = preprocessing.sklearn_wrapper.SklearnPreprocessingWrapper.load(primary_params["preprocessing_pkl_path"])
                logger.debug(f"âœ… Successfully loaded preprocessing model from {primary_params['preprocessing_pkl_path']}")
                # ç®€å•è®°å½•æ¨¡å‹ä¿¡æ¯
                model_info = f"Preprocessing model: {type(prep_model.model).__name__}"
                if hasattr(prep_model.model, 'get_params'):
                    model_info += f", params: {prep_model.model.get_params()}"
                logger.debug(f"ğŸ“Š {model_info}")
                logger.info(f"Using existing preprocessing model from {primary_params['preprocessing_pkl_path']}")
            except Exception as e:
                logger.error(f"âŒ Failed to load preprocessing model from {primary_params['preprocessing_pkl_path']}: {e}")
                raise
            
            # Apply preprocessing
            norm_feat_np = prep_model.predict(sr_features_filled[self.sr_feature_columns])
            norm_sr_feat_df = pd.DataFrame(norm_feat_np, columns=self.sr_feature_columns)
            
            # Load decomposition models
            assert "decomposition_pkl_path" in primary_params, "decomposition_pkl_path is required when using existing models"
            try:
                with open(primary_params["decomposition_pkl_path"], "rb") as f:
                    decomp_models = pickle.load(f)
                logger.debug(f"âœ… Successfully loaded decomposition models from {primary_params['decomposition_pkl_path']}")
                # ç®€å•è®°å½•æ¨¡å‹ä¿¡æ¯
                sr_ids = list(decomp_models.keys())
                model_types = [type(model.model).__name__ for model in decomp_models.values()]
                logger.debug(f"ğŸ“Š Decomposition models: {len(sr_ids)} SR regions ({sr_ids}), model types: {set(model_types)}")
                # è®°å½•ç¬¬ä¸€ä¸ªæ¨¡å‹çš„å‚æ•°ä½œä¸ºç¤ºä¾‹
                if sr_ids:
                    first_model = decomp_models[sr_ids[0]]
                    if hasattr(first_model.model, 'get_params'):
                        logger.debug(f"ğŸ“Š Sample model params: {first_model.model.get_params()}")
                logger.info(f"Using existing decomposition models from {primary_params['decomposition_pkl_path']}")
            except Exception as e:
                logger.error(f"âŒ Failed to load decomposition models from {primary_params['decomposition_pkl_path']}: {e}")
                raise
            
            # Apply decomposition for each SR
            decomp_features_list = []
            for sr_id in sr_features_filled["sr_id"].unique():
                if sr_id not in decomp_models:
                    logger.warning(f"No decomposition model found for SR ID {sr_id}, skipping")
                    continue
                
                # Get features for this SR
                sr_mask = sr_features_filled["sr_id"] == sr_id
                sr_specific_features = norm_sr_feat_df[sr_mask]
                
                # Apply decomposition
                decomp_feat = decomp_models[sr_id].predict(sr_specific_features)
                
                # Add SR ID and unique ID back
                decomp_feat_with_ids = pd.concat([
                    sr_features_filled[sr_mask][["unique_id", "sr_id"]].reset_index(drop=True),
                    decomp_feat.reset_index(drop=True)
                ], axis=1)
                decomp_features_list.append(decomp_feat_with_ids)
            
            # Combine all SR decomposed features
            all_decomp_features = pd.concat(decomp_features_list, axis=0, ignore_index=True)
            
            # Transform to wide format
            decomp_columns = [col for col in all_decomp_features.columns if col not in ["unique_id", "sr_id"]]
            result_df, _ = transform_to_wide_format(all_decomp_features, "unique_id", "sr_id", decomp_columns)
            
        result_df["unique_id"] = result_df["unique_id"].astype(int)
        result_df.sort_values(by="unique_id", inplace=True)
        case_info_df = pd.DataFrame(range(len(self.input_image_paths)), columns=["unique_id"])
        result_df = pd.merge(case_info_df, result_df, on="unique_id", how="left")
        return result_df
            
        # This should not happen due to the assert at the beginning, but just in case
        # The logic above handles both cases based on use_existing_models flag


# class RadiomicsLHCPTester:
#     """è§„èŒƒåŒ–çš„Radiomics LHCPæµ‹è¯•ç±»"""
    
#     def __init__(self, config: Dict[str, Any]):
#         """
#         åˆå§‹åŒ–æµ‹è¯•å™¨
        
#         Args:
#             config: é…ç½®å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„è·¯å¾„å’Œå‚æ•°
#         """
#         self.config = config
#         self.output_dir = Path(config.get("output_dir", "./test_output"))
#         self.output_dir.mkdir(parents=True, exist_ok=True)
        
#         # è®¾ç½®æ—¥å¿—
#         logger.info(f"RadiomicsLHCPTester initialized with output dir: {self.output_dir}")
        
#     def load_and_prepare_data(self):
#         """åŠ è½½å¹¶å‡†å¤‡æ•°æ®"""
#         logger.info("Loading and preparing data...")
        
#         # åŠ è½½æ•°æ®
#         data_csv_path = self.config["data_csv_path"]
#         base_dir = Path(self.config["base_dir"])
        
#         df = pd.read_csv(data_csv_path)
#         df["abs_image_path"] = df["image_path"].apply(lambda x: base_dir / x)
#         df["abs_mask_path"] = df["mask_path"].apply(lambda x: base_dir / x)
        
#         # æ•°æ®åˆ†å‰²
#         train_size = self.config.get("train_size", 20)
#         test_size = self.config.get("test_size", 20)
        
#         df_train = df.iloc[:train_size]
#         df_test = df.iloc[train_size:train_size + test_size]
        
#         logger.info(f"Data prepared - Total: {len(df)}, Train: {len(df_train)}, Test: {len(df_test)}")
        
#         return df_train, df_test
    
#     def create_base_params(self):
#         """åˆ›å»ºåŸºç¡€å‚æ•°é…ç½®"""
#         model_dir = self.output_dir / "models"
#         model_dir.mkdir(exist_ok=True)
        
#         return {
#             "radiomics_params": self.config["radiomics_params"],
#             "n_jobs": self.config.get("n_jobs", 24),
#             "primary_params": {
#                 "preprocessing_pkl_path": str(model_dir / "preprocessing_model.pkl"),
#                 "decomposition_pkl_path": str(model_dir / "decomposition_model.pkl"),
#                 "preprocessing": self.config.get("preprocessing", {
#                     "method": "StandardScaler",
#                     "params": None
#                 }),
#                 "decomposition": self.config.get("decomposition", {
#                     "method": "PCA", 
#                     "params": {"n_components": 10}
#                 })
#             }
#         }
    
    # def run_training_phase(self, df_train):
    #     """æ‰§è¡Œè®­ç»ƒé˜¶æ®µ"""
    #     logger.info("=== Training Phase ===")
        
    #     params = self.create_base_params()
    #     params["use_existing_models"] = False  # Train new models
        
    #     try:
    #         extractor = HiomicsRadiomicsLHCP(
    #             df_train["abs_image_path"].tolist(), 
    #             df_train["abs_mask_path"].tolist(), 
    #             **params
    #         )
            
    #         # æå–ç‰¹å¾
    #         extractor.get_radiomics()
    #         primary_features_df = extractor.get_radiomics_primary()
            
    #         # ä¿å­˜ç»“æœ
    #         results_dir = self.output_dir / "results"
    #         results_dir.mkdir(exist_ok=True)
            
    #         # åªä¿å­˜æ ¸å¿ƒç»“æœæ–‡ä»¶
            
    #         # 1. ä¿å­˜ä¸»è¦ç‰¹å¾ï¼ˆæœ€ç»ˆç»“æœ - ç”¨æˆ·æœ€éœ€è¦çš„ï¼‰
    #         train_results_path = results_dir / "primary_features_train.csv"
    #         primary_features_df.to_csv(train_results_path, index=False)
            
    #         # 2. ä¿å­˜è®­ç»ƒæ•°æ®ä¿¡æ¯ï¼ˆä¾¿äºè¿½æº¯ï¼‰
    #         train_info_path = results_dir / "train_info.csv"
    #         df_train[["abs_image_path", "abs_mask_path"]].to_csv(train_info_path, index=False)
            
    #         logger.info(f"Training completed - Features: {primary_features_df.shape[1]-1}, Samples: {primary_features_df.shape[0]}")
    #         logger.info(f"âœ… ä¸»è¦ç‰¹å¾æ–‡ä»¶: {train_results_path}")
    #         logger.info(f"ğŸ“‹ è®­ç»ƒæ•°æ®ä¿¡æ¯: {train_info_path}")
            
    #         return primary_features_df, extractor
            
    #     except Exception as e:
    #         logger.error(f"Training phase failed: {str(e)}")
    #         raise
    
    # def run_testing_phase(self, df_test):
    #     """æ‰§è¡Œæµ‹è¯•é˜¶æ®µ"""
    #     logger.info("=== Testing Phase ===")
        
    #     params = self.create_base_params()
    #     params["use_existing_models"] = True  # Use existing models
    #     # ä½¿ç”¨å·²æœ‰æ¨¡å‹åªéœ€è¦æ¨¡å‹è·¯å¾„
    #     params["primary_params"] = {
    #         "preprocessing_pkl_path": params["primary_params"]["preprocessing_pkl_path"],
    #         "decomposition_pkl_path": params["primary_params"]["decomposition_pkl_path"]
    #     }
        
    #     try:
    #         extractor = HiomicsRadiomicsLHCP(
    #             df_test["abs_image_path"].tolist(), 
    #             df_test["abs_mask_path"].tolist(), 
    #             **params
    #         )
            
    #         # æå–ç‰¹å¾
    #         extractor.get_radiomics()
    #         primary_features_df = extractor.get_radiomics_primary()
            
    #         # ä¿å­˜ç»“æœ
    #         results_dir = self.output_dir / "results"
    #         results_dir.mkdir(exist_ok=True)
            
    #         # åªä¿å­˜æ ¸å¿ƒç»“æœæ–‡ä»¶
            
    #         # 1. ä¿å­˜ä¸»è¦ç‰¹å¾ï¼ˆæœ€ç»ˆç»“æœ - ç”¨æˆ·æœ€éœ€è¦çš„ï¼‰
    #         test_results_path = results_dir / "primary_features_test.csv"
    #         primary_features_df.to_csv(test_results_path, index=False)
            
    #         # 2. ä¿å­˜æµ‹è¯•æ•°æ®ä¿¡æ¯ï¼ˆä¾¿äºè¿½æº¯ï¼‰
    #         test_info_path = results_dir / "test_info.csv"
    #         df_test[["abs_image_path", "abs_mask_path"]].to_csv(test_info_path, index=False)
            
    #         logger.info(f"Testing completed - Features: {primary_features_df.shape[1]-1}, Samples: {primary_features_df.shape[0]}")
    #         logger.info(f"âœ… ä¸»è¦ç‰¹å¾æ–‡ä»¶: {test_results_path}")
    #         logger.info(f"ğŸ“‹ æµ‹è¯•æ•°æ®ä¿¡æ¯: {test_info_path}")
            
    #         return primary_features_df
            
    #     except Exception as e:
    #         logger.error(f"Testing phase failed: {str(e)}")
    #         raise
    
    # def compare_results(self, train_df, test_df):
    #     """æ¯”è¾ƒè®­ç»ƒå’Œæµ‹è¯•ç»“æœ"""
    #     logger.info("=== Feature Statistics Comparison ===")
        
    #     try:
    #         # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    #         train_features = train_df.iloc[:, 1:]  # æ’é™¤unique_idåˆ—
    #         test_features = test_df.iloc[:, 1:]    # æ’é™¤unique_idåˆ—
            
    #         train_stats = {
    #             "mean": train_features.mean().mean(),
    #             "std": train_features.std().mean(),
    #             "min": train_features.min().min(),
    #             "max": train_features.max().max()
    #         }
            
    #         test_stats = {
    #             "mean": test_features.mean().mean(),
    #             "std": test_features.std().mean(),
    #             "min": test_features.min().min(),
    #             "max": test_features.max().max()
    #         }
            
    #         # è®°å½•ç»Ÿè®¡ä¿¡æ¯
    #         logger.info(f"Train Stats - Mean: {train_stats['mean']:.4f}, Std: {train_stats['std']:.4f}, "
    #                    f"Min: {train_stats['min']:.4f}, Max: {train_stats['max']:.4f}")
    #         logger.info(f"Test Stats  - Mean: {test_stats['mean']:.4f}, Std: {test_stats['std']:.4f}, "
    #                    f"Min: {test_stats['min']:.4f}, Max: {test_stats['max']:.4f}")
            
    #         # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    #         stats_df = pd.DataFrame({
    #             "phase": ["train", "test"],
    #             "mean": [train_stats["mean"], test_stats["mean"]],
    #             "std": [train_stats["std"], test_stats["std"]],
    #             "min": [train_stats["min"], test_stats["min"]],
    #             "max": [train_stats["max"], test_stats["max"]]
    #         })
            
    #         stats_path = self.output_dir / "results" / "feature_statistics.csv"
    #         stats_df.to_csv(stats_path, index=False)
    #         logger.info(f"Statistics saved to: {stats_path}")
            
    #     except Exception as e:
    #         logger.error(f"Statistics comparison failed: {str(e)}")
    
    # def run_full_test(self):
    #     """æ‰§è¡Œå®Œæ•´çš„æµ‹è¯•æµç¨‹"""
    #     logger.info("Starting full Radiomics LHCP test...")
        
    #     try:
    #         # åŠ è½½æ•°æ®
    #         df_train, df_test = self.load_and_prepare_data()
            
    #         # è®­ç»ƒé˜¶æ®µ
    #         train_results, _ = self.run_training_phase(df_train)
            
    #         # æµ‹è¯•é˜¶æ®µ
    #         test_results = self.run_testing_phase(df_test)
            
    #         # æ¯”è¾ƒç»“æœ
    #         self.compare_results(train_results, test_results)
            
    #         # æ¸…æ™°æ€»ç»“æœ€ç»ˆè¾“å‡ºæ–‡ä»¶
    #         logger.info("="*60)
    #         logger.info("ğŸ‰ å®Œæ•´æµ‹è¯•å®Œæˆï¼æœ€ç»ˆè¾“å‡ºæ–‡ä»¶æ€»ç»“ï¼š")
    #         logger.info("="*60)
    #         logger.info("ğŸ“ æ ¸å¿ƒç»“æœæ–‡ä»¶ (ç”¨æˆ·ä¸»è¦å…³æ³¨):")
    #         logger.info(f"   âœ… è®­ç»ƒé›†ç‰¹å¾: {self.output_dir}/results/primary_features_train.csv")
    #         logger.info(f"   âœ… æµ‹è¯•é›†ç‰¹å¾: {self.output_dir}/results/primary_features_test.csv")
    #         logger.info("")
    #         logger.info("ğŸ“ æ•°æ®ä¿¡æ¯æ–‡ä»¶ (ä¾¿äºè¿½æº¯):")
    #         logger.info(f"   ğŸ“‹ è®­ç»ƒæ•°æ®ä¿¡æ¯: {self.output_dir}/results/train_info.csv")
    #         logger.info(f"   ğŸ“‹ æµ‹è¯•æ•°æ®ä¿¡æ¯: {self.output_dir}/results/test_info.csv")
    #         logger.info("")
    #         logger.info("ğŸ“ ç»Ÿè®¡åˆ†ææ–‡ä»¶:")
    #         logger.info(f"   ğŸ“Š ç‰¹å¾ç»Ÿè®¡æ¯”è¾ƒ: {self.output_dir}/results/feature_statistics.csv")
    #         logger.info("")
    #         logger.info("ğŸ“ æ¨¡å‹æ–‡ä»¶:")
    #         logger.info(f"   ğŸ”§ é¢„å¤„ç†æ¨¡å‹: {self.output_dir}/models/preprocessing_model.pkl")
    #         logger.info(f"   ğŸ”§ é™ç»´æ¨¡å‹: {self.output_dir}/models/decomposition_model.pkl")
    #         logger.info("="*60)
    #         logger.info("ğŸ’¡ å»ºè®®: ä¸»è¦æŸ¥çœ‹ primary_features_train.csv å’Œ primary_features_test.csv")
    #         logger.info("="*60)
            
    #     except Exception as e:
    #         logger.error(f"Full test failed: {str(e)}")
    #         raise


# def create_default_test_config():
#     """åˆ›å»ºé»˜è®¤çš„æµ‹è¯•é…ç½®"""
#     return {
#         "data_csv_path": "/media/wzt/plum14t/PyHiomics/exp/pCR/04_Clu_Rad/output/val/data/04_Clu_Rad/data.csv",
#         "base_dir": "/media/wzt/plum14t/PyHiomics/exp/pCR/04_Clu_Rad/output/val/data/04_Clu_Rad",
#         "radiomics_params": "/media/wzt/plum14t/PyHiomics/exp/pCR/03_FE_Rad/MR_noshape.yaml",
#         "output_dir": "./radiomics_lhcp_test_output",
#         "n_jobs": 24,
#         "train_size": 20,
#         "test_size": 20,
#         "preprocessing": {
#             "method": "StandardScaler",
#             "params": None
#         },
#         "decomposition": {
#             "method": "PCA", 
#             "params": {"n_components": 1}
#         }
#     }


# if __name__ == "__main__":
#     # åˆ›å»ºæµ‹è¯•é…ç½®
#     config = create_default_test_config()
    
#     # åˆ›å»ºæµ‹è¯•å™¨å¹¶è¿è¡Œ
#     tester = RadiomicsLHCPTester(config)
#     tester.run_full_test()


class RadiomicsLHCPStep(AbcStep):
    """RadiomicsLHCPç‰¹å¾æå–æ­¥éª¤ç±»"""
    
    def __init__(self, params=None):
        if params is None:
            params = "RadiomicsLHCP.yaml"
            logger.info(f"Using default params for RadiomicsLHCP: {params}")
        self.params = load_params(params)
        self.enabled_features = self.params.get("enabled_features", ["base", "primary"])
        self.radiomics_params = self.params.get("radiomics_params")
        self.primary_params = self.params.get("primary_params", {})
        
    def to_kwargs(self):
        return {"params": self.params}
        
    def __eq__(self, other):
        return self.params == other.params
        
    def save(self, step_dir, input_map, result_dir):
        tmp = {
            "step_obj": self,
            "input_map": input_map,
            "result_dir": result_dir,
        }
        with open(step_dir / "step.pkl", "wb") as f:
            pickle.dump(tmp, f)
        
    def run(self, step_dir, result_dir, path_data: PathData, force=False, n_jobs=1, **kwargs):
        # logger.debug("@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@")
        # path_data.df = path_data.df.head(10)

        """è¿è¡ŒRadiomicsLHCPç‰¹å¾æå–æ­¥éª¤"""
        step_dir = Path(step_dir)
        result_dir = Path(result_dir)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è¿è¡Œ
        if not force:
            step_pkl = step_dir / "step.pkl"
            data_pkl = result_dir / "data.pkl"
            if step_pkl.exists() and data_pkl.exists():
                with open(data_pkl, "rb") as f:
                    data_obj = pickle.load(f)
                    # ç®€å•è®°å½•åŠ è½½çš„æ•°æ®ä¿¡æ¯
                    if hasattr(data_obj, 'feature_columns') and data_obj.feature_columns:
                        logger.debug(f"ğŸ“Š Loaded feature data: {len(data_obj.feature_columns)} features")
                        logger.debug(f"ğŸ“Š Sample features: {data_obj.feature_columns[:5]}..." if len(data_obj.feature_columns) > 5 else f"ğŸ“Š Features: {data_obj.feature_columns}")
                    logger.info(f"Loaded RadiomicsLHCP from {data_pkl}")
                    return data_obj
        
        # è·å–è¾“å…¥æ•°æ®è·¯å¾„
        image_paths = path_data.get_images()
        mask_paths = path_data.get_masks()
        
        logger.info(f"Processing {len(image_paths)} cases with RadiomicsLHCP")
        
        try:
            # åŠ¨æ€è®¾ç½®æ¨¡å‹ä¿å­˜è·¯å¾„
            primary_params = self.primary_params.copy() if self.primary_params else {}
            if primary_params.get("preprocessing_pkl_path") is None:
                primary_params["preprocessing_pkl_path"] = str(step_dir / "preprocessing_model.pkl")
            if primary_params.get("decomposition_pkl_path") is None:
                primary_params["decomposition_pkl_path"] = str(step_dir / "decomposition_model.pkl")
            
            # åŠ¨æ€å†³å®šæ˜¯å¦ä½¿ç”¨å·²æœ‰æ¨¡å‹ï¼šåŸºäºpklæ–‡ä»¶å­˜åœ¨ä¸å¦å’Œforceå‚æ•°
            preprocessing_pkl_path = Path(primary_params["preprocessing_pkl_path"])
            decomposition_pkl_path = Path(primary_params["decomposition_pkl_path"])
            
            # å¦‚æœpklæ–‡ä»¶å­˜åœ¨ä¸”ä¸å¼ºåˆ¶é‡æ–°è®­ç»ƒï¼Œä½¿ç”¨å·²æœ‰æ¨¡å‹ï¼›å¦åˆ™è®­ç»ƒæ–°æ¨¡å‹
            if (preprocessing_pkl_path.exists() and decomposition_pkl_path.exists() and not force):
                should_train_new = False
                logger.info(f"ğŸ”„ Found existing models, will load and reuse - preprocessing: {preprocessing_pkl_path}, decomposition: {decomposition_pkl_path}")
            else:
                should_train_new = True
                logger.info(f"ğŸ—ï¸ Will train new models - force={force}, prep_exists={preprocessing_pkl_path.exists()}, decomp_exists={decomposition_pkl_path.exists()}")
            
            # åˆ›å»ºå‚æ•°å­—å…¸
            extractor_params = {
                "radiomics_params": self.radiomics_params,
                "should_train_new": should_train_new,
                "n_jobs": n_jobs
            }
            
            # å¦‚æœæœ‰primary_paramsï¼Œæ·»åŠ åˆ°å‚æ•°ä¸­
            if primary_params:
                extractor_params["primary_params"] = primary_params
            
            # åˆ›å»ºRadiomicsLHCPæå–å™¨
            extractor = HiomicsRadiomicsLHCP(
                input_image_paths=image_paths,
                input_mask_paths=mask_paths,
                **extractor_params
            )
            
            results = {}
            all_features = []
            feature_columns = []
            
            # æ ¹æ®enabled_featuresæ¥å†³å®šæå–å“ªäº›ç‰¹å¾
            if "base" in self.enabled_features:
                base_features = extractor.get_base()
                results["base"] = base_features
            
            if "primary" in self.enabled_features and self.primary_params:
                primary_features = extractor.get_primary()
                results["primary"] = primary_features
            
            # å¤„ç†ç»“æœ
            for method_name, method_df in results.items():
                if not method_df.empty:
                    # è¯†åˆ«IDåˆ—ï¼ˆå¯èƒ½æ˜¯unique_idæˆ–å…¶ä»–åç§°ï¼‰
                    id_cols_to_drop = [col for col in method_df.columns 
                                     if col in ["unique_id", path_data.id_column]]
                    
                    # è·å–ç‰¹å¾åˆ—ï¼ˆæ’é™¤IDåˆ—ï¼‰
                    feature_cols = [col for col in method_df.columns if col not in id_cols_to_drop]
                    
                    # æ·»åŠ åŸå§‹æ•°æ®ä¿¡æ¯
                    if feature_cols:  # ç¡®ä¿æœ‰ç‰¹å¾åˆ—
                        # é‡å‘½åç‰¹å¾åˆ—ï¼Œæ·»åŠ RadLHCP@å‰ç¼€
                        renamed_df = method_df[feature_cols].copy()
                        for col in feature_cols:
                            renamed_df.rename(columns={col: f"RadLHCP@{method_name}@{col}"}, inplace=True)
                        
                        df_with_info = pd.concat([
                            path_data.df.reset_index(drop=True),
                            renamed_df.reset_index(drop=True)
                        ], axis=1)
                        all_features.append(df_with_info)
                        feature_columns.extend([f"RadLHCP@{method_name}@{col}" for col in feature_cols])
            
            # åˆå¹¶æ‰€æœ‰ç‰¹å¾
            if all_features:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªDataFrameä½œä¸ºåŸºç¡€ï¼Œç„¶åæ·»åŠ å…¶ä»–ç‰¹å¾
                final_df = all_features[0].copy()
                for i in range(1, len(all_features)):
                    # åªæ·»åŠ ç‰¹å¾åˆ—ï¼Œé¿å…é‡å¤ä¿¡æ¯åˆ—
                    feature_cols = [col for col in all_features[i].columns 
                                  if col not in final_df.columns]
                    if feature_cols:
                        final_df = pd.concat([final_df, all_features[i][feature_cols]], axis=1)
                
                # æ›´æ–°feature_columnsä¸ºå®é™…å­˜åœ¨çš„åˆ—
                feature_columns = [col for col in final_df.columns 
                                 if col not in path_data.df.columns]
            else:
                final_df = path_data.df.copy()
                feature_columns = []
            
            # ä¿å­˜ç»“æœ
            step_dir.mkdir(parents=True, exist_ok=True)
            result_dir.mkdir(parents=True, exist_ok=True)
            
            # å¤‡ä»½radiomicså‚æ•°æ–‡ä»¶åˆ°step_dir
            if self.radiomics_params and Path(self.radiomics_params).exists():
                import shutil
                radiomics_param_file = Path(self.radiomics_params)
                backup_param_file = step_dir / f"radiomics_params_{radiomics_param_file.name}"
                shutil.copy2(self.radiomics_params, backup_param_file)
                logger.debug(f"ğŸ“‹ Backed up radiomics params to {backup_param_file}")
            
            with open(step_dir / "step.pkl", "wb") as f:
                pickle.dump({"step_obj": self}, f)
                
            # å…ˆä¿å­˜CSVæ–‡ä»¶
            csv_path = result_dir / "data.csv"
            final_df.to_csv(csv_path, index=False)
            
            # åˆ›å»ºç‰¹å¾æ•°æ®å¯¹è±¡
            feature_data = FeatureData(
                csv_path=str(csv_path),
                feature_columns=feature_columns,
                id_column=path_data.id_column
            )
                
            with open(result_dir / "data.pkl", "wb") as f:
                pickle.dump(feature_data, f)
            
            logger.info(f"RadiomicsLHCP extraction completed - Features: {len(feature_columns)}, Samples: {len(final_df)}")
            
            return feature_data
            
        except Exception as e:
            logger.error(f"RadiomicsLHCP extraction failed: {str(e)}")
            raise